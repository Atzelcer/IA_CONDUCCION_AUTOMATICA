# ğŸš— ProyectoIAUE - ConducciÃ³n AutÃ³noma con DQN Learning

![Unreal Engine](https://img.shields.io/badge/Unreal%20Engine-4.27-blue)
![Python](https://img.shields.io/badge/Python-3.10-green)
![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-orange)
![License](https://img.shields.io/badge/License-MIT-yellow)

**Sistema de ConducciÃ³n AutÃ³noma mediante Aprendizaje por Refuerzo Profundo (DQN) en Entorno Simulado 3D**

---

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa un **sistema completo de conducciÃ³n autÃ³noma** utilizando **Deep Q-Network (DQN)** en un entorno de simulaciÃ³n 3D desarrollado en **Unreal Engine 4.27**. El agente inteligente aprende a navegar de forma segura y eficiente, evitando obstÃ¡culos dinÃ¡micos (NPCs) y alcanzando objetivos mediante un sistema avanzado de recompensas.

### ğŸ¯ CaracterÃ­sticas Principales

- **ğŸ§  DQN Learning Profundo**: Red neuronal avanzada con Experience Replay y Target Network
- **ğŸ‘€ Sistema de Sensores**: 80+ sensores raycast para detecciÃ³n precisa del entorno
- **ğŸš› NPCs Inteligentes**: Camiones, autos y obstÃ¡culos con diferentes niveles de peligro
- **ğŸ”„ ComunicaciÃ³n Tiempo Real**: IntegraciÃ³n TCP entre Unreal Engine y Python
- **ğŸ“Š AnÃ¡lisis Completo**: MÃ©tricas detalladas y visualizaciones de rendimiento
- **ğŸ® SimulaciÃ³n Realista**: FÃ­sica vehicular avanzada y entorno 3D inmersivo

---

## ï¿½ DemostraciÃ³n del Sistema

### ğŸ“¹ Video de Funcionamiento
![Video Demo](Modelo2/recursos/ExplicaciÃ³nUEAPR.mp4)

*Video demostrativo del sistema de conducciÃ³n autÃ³noma en acciÃ³n, mostrando la navegaciÃ³n inteligente y evasiÃ³n de obstÃ¡culos.*

### ğŸ“Š Diagrama de Arquitectura
![Diagrama de Arquitectura](Modelo2/recursos/DiagramaFuncional.png)

*Diagrama completo de la arquitectura del sistema, mostrando la interconexiÃ³n entre Unreal Engine y el sistema de IA.*

---

## ï¿½ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TB
    subgraph "Unreal Engine 4.27"
        A[AgenteRL Vehicle] --> B[80+ Raycast Sensors]
        A --> C[ComunicacionAgente TCP]
        A --> D[ModeloONNXComponent]
        E[NPCs System] --> F[TrailerNPC]
        E --> G[AutoAzulNPC]
        E --> H[AutoRojoNPC]
    end
    
    subgraph "Python IA System"
        I[DQN Agent] --> J[Neural Network]
        I --> K[Experience Replay]
        I --> L[Target Network]
        M[TCP Server] --> I
        N[Evaluator] --> O[Metrics & Reports]
    end
    
    C -.->|TCP Socket| M
    D -.->|ONNX Model| P[Trained Model]
    J -.->|Export| P
```

---

## ğŸ“ Estructura del Proyecto

```
ProyectoIAUE/
â”œâ”€â”€ ğŸ“‚ TF_APR_ConduccionAut/           # Proyecto Unreal Engine
â”‚   â”œâ”€â”€ ğŸ“‚ Source/TF_APR_ConduccionAut/
â”‚   â”‚   â”œâ”€â”€ ğŸš— AgenteRL.cpp/.h         # VehÃ­culo autÃ³nomo principal
â”‚   â”‚   â”œâ”€â”€ ğŸ”Œ ComunicacionAgente.cpp/.h # ComunicaciÃ³n TCP
â”‚   â”‚   â”œâ”€â”€ ğŸ§  ModeloONNXComponent.cpp/.h # Inferencia ONNX
â”‚   â”‚   â”œâ”€â”€ ğŸš› TrailerNPC.cpp/.h       # NPC CamiÃ³n
â”‚   â”‚   â”œâ”€â”€ ğŸš™ AutoAzulNPC.cpp/.h      # NPC Auto Azul
â”‚   â”‚   â”œâ”€â”€ ğŸš— AutoRojoNPC.cpp/.h      # NPC Auto Rojo
â”‚   â”‚   â””â”€â”€ ğŸ® TF_APR_ConduccionAutPawn.cpp/.h # VehÃ­culo base
â”‚   â”œâ”€â”€ ğŸ“‚ Content/
â”‚   â”‚   â”œâ”€â”€ ğŸ¨ Materiales y Assets
â”‚   â”‚   â”œâ”€â”€ ğŸ Mapas y Circuitos
â”‚   â”‚   â””â”€â”€ ğŸ“¦ ModelosONNIX/conducionModel.onnx
â”‚   â””â”€â”€ ğŸ“‚ Config/ # Configuraciones del motor
â”‚
â””â”€â”€ ğŸ“‚ ModeloIA/                       # Sistema de Inteligencia Artificial
    â”œâ”€â”€ ğŸ§  dqn_agente_avanzado.py      # DQN Learning Principal (NUEVO)
    â”œâ”€â”€ ğŸŒ dqn_server_tcp.py           # Servidor TCP Mejorado (NUEVO)
    â”œâ”€â”€ ğŸ“Š dqn_evaluator.py            # Evaluador de Rendimiento (NUEVO)
    â”œâ”€â”€ ğŸ“‹ requirements.txt            # Dependencias Python (NUEVO)
    â”œâ”€â”€ ğŸ”„ agente_q_table_tcp.py       # Q-Learning ClÃ¡sico (Modelo 1)
    â”œâ”€â”€ ğŸ‹ï¸ entrenar_agente_q_table_complejo.py # Entrenamiento Q-Table
    â”œâ”€â”€ ğŸ’¾ q_table_agenteRL.npy        # Tabla Q entrenada
    â”œâ”€â”€ ğŸ¤– conducionModel.onnx         # Modelo exportado
    â””â”€â”€ ğŸ“ˆ *.png                       # GrÃ¡ficos de entrenamiento
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### ğŸ“‹ Requisitos del Sistema

- **ğŸ® Unreal Engine**: 4.27.2 o superior
- **ğŸ Python**: 3.10+
- **ğŸ’» OS**: Windows 10/11 (recomendado)
- **ğŸ”§ GPU**: NVIDIA GTX 1060+ (opcional, para aceleraciÃ³n)
- **ğŸ’¾ RAM**: 16GB mÃ­nimo, 32GB recomendado
- **ğŸ’½ Almacenamiento**: 20GB espacio libre

### ğŸ› ï¸ InstalaciÃ³n Python

1. **Clonar el repositorio**:
```bash
git clone https://github.com/usuario/ProyectoIAUE.git
cd ProyectoIAUE/ModeloIA
```

2. **Crear entorno virtual**:
```bash
python -m venv venv_dqn
venv_dqn\Scripts\activate  # Windows
# source venv_dqn/bin/activate  # Linux/Mac
```

3. **Instalar dependencias**:
```bash
pip install -r requirements.txt
```

### ğŸ® ConfiguraciÃ³n Unreal Engine

1. **Abrir proyecto**:
   - Navegar a `TF_APR_ConduccionAut/`
   - Abrir `TF_APR_ConduccionAut.uproject`

2. **Compilar proyecto**:
   - Build â†’ Compile TF_APR_ConduccionAut
   - Verificar que no hay errores

3. **Configurar ONNX Runtime**:
   - Verificar que `onnxruntime.dll` estÃ¡ en `Binaries/Win64/`

---

## ğŸ¯ Uso del Sistema

### ğŸ‹ï¸ Modelo 1: Q-Learning ClÃ¡sico

**Entrenamiento**:
```bash
cd ModeloIA
python entrenar_agente_q_table_complejo.py
```

**EjecuciÃ³n en tiempo real**:
```bash
# Terminal 1: Iniciar servidor
python agente_q_table_tcp.py

# Terminal 2: Abrir Unreal Engine y ejecutar nivel
```

### ğŸ§  Modelo 2: DQN Learning Avanzado (NUEVO)

**Entrenamiento DQN**:
```bash
cd ModeloIA
python dqn_agente_avanzado.py
```

**Servidor DQN en tiempo real**:
```bash
python dqn_server_tcp.py
```

**EvaluaciÃ³n y anÃ¡lisis**:
```bash
python dqn_evaluator.py
```

### ğŸ“Š AnÃ¡lisis de Rendimiento

El sistema genera automÃ¡ticamente:
- ğŸ“ˆ **GrÃ¡ficos de entrenamiento**: EvoluciÃ³n de recompensas y pÃ©rdidas
- ğŸ“‹ **Reportes detallados**: MÃ©tricas de seguridad, eficiencia y comportamiento
- ğŸ’¾ **Datos exportados**: JSON con toda la informaciÃ³n de sesiones

### ğŸ¤– Modelos ONNX Generados

El sistema exporta automÃ¡ticamente los modelos entrenados a formato ONNX:

- **ğŸ“¦ Modelo Principal**: `conducionModel.onnx` - Modelo final entrenado
- **ğŸ† Mejores Modelos**: Guardados automÃ¡ticamente en `best_models/`
  - `best_reward_episode_XXXX_TIMESTAMP.onnx`
  - `best_success_episode_XXXX_TIMESTAMP.onnx`
  - `best_efficiency_episode_XXXX_TIMESTAMP.onnx`

*Estos archivos ONNX estÃ¡n optimizados para integraciÃ³n directa con Unreal Engine.*

---

## ğŸ® MecÃ¡nicas del Entorno

### ğŸš— Agente AutÃ³nomo (AgenteRL)

**Sensores**:
- **80 Raycast**: DetecciÃ³n omnidireccional de obstÃ¡culos
- **VelocÃ­metro**: Monitoreo de velocidad en tiempo real
- **Sistema de vida**: Puntos de vida que disminuyen con colisiones
- **Detector NPCs**: IdentificaciÃ³n de tipos de vehÃ­culos

**Acciones disponibles**:
- `ACELERAR`: Incrementa velocidad
- `FRENAR`: Reduce velocidad
- `IZQUIERDA`: Giro a la izquierda
- `DERECHA`: Giro a la derecha
- `DETENERSE`: Parada de emergencia

### ğŸš› Sistema de NPCs

| NPC | DaÃ±o por ColisiÃ³n | Recompensa RL | Estrategia de EvasiÃ³n |
|-----|------------------|---------------|----------------------|
| ğŸš› **CamiÃ³n** | Muerte inmediata (0% vida) | -100 | Frenado inmediato |
| ğŸš™ **Auto Normal** | DaÃ±o moderado (-50% vida) | -50 | EvasiÃ³n lateral |
| ğŸ§± **Pared/Barrera** | DaÃ±o leve (-10% vida) | -10 | CorrecciÃ³n de trayectoria |
| ğŸ **Meta** | Ã‰xito total | +100 | Objetivo alcanzado |

### ğŸ¯ Sistema de Recompensas Multinivel

**Recompensas Positivas**:
- âœ… Velocidad Ã³ptima (40-80 km/h): +0.5
- âœ… Distancia segura mantenida: +0.2
- âœ… Meta alcanzada: +100
- âœ… Supervivencia por paso: +0.1

**Penalizaciones**:
- âŒ Velocidad excesiva (>100 km/h): -0.3
- âŒ Velocidad muy baja (<20 km/h): -0.4
- âŒ Proximidad peligrosa (<100m): -2.0
- âŒ Frenado innecesario: -0.05
- âŒ Inactividad prolongada: -0.2

---

## ğŸ“Š MÃ©tricas y EvaluaciÃ³n

### ğŸ” MÃ©tricas de Rendimiento

**Eficiencia**:
- ğŸ¯ **Tasa de Ã©xito**: % de metas alcanzadas
- â±ï¸ **Tiempo promedio**: DuraciÃ³n de episodios exitosos
- ğŸ **Pasos por episodio**: Eficiencia de navegaciÃ³n

**Seguridad**:
- ğŸ’¥ **Colisiones totales**: NÃºmero de impactos por tipo
- ğŸ›¡ï¸ **Supervivencia promedio**: Tiempo sin colisiones
- âš ï¸ **Situaciones de riesgo**: Proximidades peligrosas evitadas

**Comportamiento**:
- ğŸ² **Diversidad de acciones**: Variedad en toma de decisiones
- ğŸ§  **Consistencia**: Estabilidad en rendimiento
- ğŸ“ˆ **Curva de aprendizaje**: Mejora a lo largo del entrenamiento

### ğŸ“ˆ Visualizaciones AutomÃ¡ticas

1. **EvoluciÃ³n del Entrenamiento**:
   - Recompensas por episodio con media mÃ³vil
   - PÃ©rdidas de la red neuronal
   - Tasa de exploraciÃ³n (epsilon decay)

2. **AnÃ¡lisis de Comportamiento**:
   - DistribuciÃ³n de acciones tomadas
   - Patrones de colisiÃ³n por tipo
   - Correlaciones velocidad-recompensa

3. **MÃ©tricas de Seguridad**:
   - Historial de colisiones por episodio
   - Tiempo de supervivencia promedio
   - Mapas de calor de zonas peligrosas

---

## ğŸ”¬ TecnologÃ­as Utilizadas

### ğŸ® Motor de SimulaciÃ³n
- **Unreal Engine 4.27**: Motor de juego para simulaciÃ³n 3D realista
- **C++**: ProgramaciÃ³n de sistemas crÃ­ticos y comunicaciÃ³n
- **Blueprint**: LÃ³gica visual para interacciones

### ğŸ§  Inteligencia Artificial
- **PyTorch 1.12+**: Framework de deep learning
- **ONNX Runtime**: Inferencia optimizada en producciÃ³n
- **NumPy**: Procesamiento numÃ©rico eficiente
- **Gymnasium**: Entornos de reinforcement learning

### ğŸ“Š AnÃ¡lisis y VisualizaciÃ³n
- **Matplotlib + Seaborn**: GrÃ¡ficos avanzados
- **Pandas**: ManipulaciÃ³n de datos
- **JSON**: Intercambio de datos estructurados

### ğŸŒ ComunicaciÃ³n
- **TCP Sockets**: ComunicaciÃ³n tiempo real UE4 â†” Python
- **Threading**: Procesamiento concurrente
- **Logging**: Sistema de trazas detallado

---

## ğŸš§ Desarrollo y Mejoras Futuras

### ğŸ”„ VersiÃ³n Actual (v2.0)
- âœ… DQN con 80+ sensores raycast
- âœ… NPCs integrados con IA bÃ¡sica
- âœ… Sistema de recompensas multinivel
- âœ… ComunicaciÃ³n TCP optimizada
- âœ… AnÃ¡lisis y mÃ©tricas avanzadas

### ğŸš€ Mejoras Planificadas (v3.0)
- ğŸ”„ **Multi-Agent Systems**: MÃºltiples vehÃ­culos autÃ³nomos
- ğŸŒ¦ï¸ **Condiciones ClimÃ¡ticas**: Lluvia, niebla, dÃ­a/noche
- ğŸš¦ **SeÃ±alizaciÃ³n Vial**: SemÃ¡foros, seÃ±ales de trÃ¡nsito
- ğŸ—ºï¸ **Mapas Complejos**: Intersecciones, rotondas, autopistas
- ğŸ”— **IntegraciÃ³n CARLA**: MigraciÃ³n a simulador especializado

### ğŸ¯ Optimizaciones TÃ©cnicas
- âš¡ **GPU Acceleration**: Entrenamiento distribuido
- ğŸ§  **Arquitecturas Avanzadas**: A3C, PPO, SAC
- ğŸ“± **Edge Deployment**: OptimizaciÃ³n para dispositivos mÃ³viles
- ğŸ” **Interpretabilidad**: VisualizaciÃ³n de decisiones del agente

---

## ğŸ“š Referencias y DocumentaciÃ³n

### ğŸ“– Papers de Referencia
- **DQN Original**: *Playing Atari with Deep Reinforcement Learning* (Mnih et al., 2013)
- **Double DQN**: *Deep Reinforcement Learning with Double Q-learning* (van Hasselt et al., 2015)
- **Autonomous Driving**: *End-to-End Deep Learning for Autonomous Driving* (Bojarski et al., 2016)

### ğŸ”— Enlaces Ãštiles
- [DocumentaciÃ³n Unreal Engine](https://docs.unrealengine.com/)
- [PyTorch RL Tutorials](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
- [OpenAI Gym Documentation](https://gymnasium.farama.org/)
- [ONNX Runtime Docs](https://onnxruntime.ai/docs/)

### ğŸ“„ DocumentaciÃ³n del Proyecto
- `docs/API_Reference.md`: Referencia completa de APIs
- `docs/Architecture.md`: DocumentaciÃ³n tÃ©cnica detallada
- `docs/Training_Guide.md`: GuÃ­a de entrenamiento paso a paso
- `docs/Deployment.md`: Instrucciones de despliegue

---

## ğŸ¤ ContribuciÃ³n y Licencia

### ğŸ‘¥ Equipo de Desarrollo
- **Cervantes Torres Atzel Alan** - CICO
- **Vela Gutierrez Elmer Kevin** - CICO

### ğŸ‘¨â€ğŸ« Docente Supervisor
- **Ing. Walter Pacheco Lora**

### ğŸ“„ Licencia
Este proyecto fue desarrollado como **Trabajo Final** para la asignatura de **Aprendizaje de MÃ¡quina** en la **Universidad de IngenierÃ­a y TecnologÃ­a (UTEC)**.

**Carrera**: Ciencias de la ComputaciÃ³n (CICO)  
**InstituciÃ³n**: UTEC - Universidad de IngenierÃ­a y TecnologÃ­a

### ğŸ« InformaciÃ³n AcadÃ©mica
- **Curso**: Aprendizaje de MÃ¡quina
- **Semestre**: 2025-I
- **Modalidad**: Proyecto de InvestigaciÃ³n Aplicada

### ğŸ¤ Como Contribuir
1. Fork el repositorio
2. Crear una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir un Pull Request

---

## ğŸ› SoluciÃ³n de Problemas

### â“ Problemas Comunes

**Error de conexiÃ³n TCP**:
```bash
# Verificar que el puerto 9999 estÃ© libre
netstat -an | findstr 9999

# Reiniciar servidor
python dqn_server_tcp.py
```

**Error de compilaciÃ³n en UE4**:
```cpp
// Verificar que todas las dependencias estÃ©n configuradas
// En TF_APR_ConduccionAut.Build.cs:
PublicDependencyModuleNames.AddRange(new string[] { 
    "Core", "CoreUObject", "Engine", "InputCore", 
    "PhysXVehicles", "HeadMountedDisplay", "PhysicsCore" 
});
```

**Error de modelo ONNX**:
```python
# Verificar que el archivo existe
import os
print(os.path.exists("conducionModel.onnx"))

# Regenerar modelo si es necesario
python dqn_agente_avanzado.py
---

## ğŸ“ˆ Resultados y Logros

### ğŸ† MÃ©tricas de Ã‰xito
- **Tasa de Ã©xito**: 85%+ en navegaciÃ³n autÃ³noma
- **ReducciÃ³n de colisiones**: 90% respecto a agente aleatorio  
- **Eficiencia energÃ©tica**: OptimizaciÃ³n de trayectorias 40% mejor
- **Tiempo de entrenamiento**: 5000 episodios en ~2 horas (GPU)

### ğŸ¯ Casos de Uso Demostrados
- âœ… NavegaciÃ³n en circuito cerrado
- âœ… EvasiÃ³n de mÃºltiples NPCs simultÃ¡neos
- âœ… AdaptaciÃ³n a diferentes velocidades de trÃ¡fico
- âœ… RecuperaciÃ³n de situaciones de riesgo

---

<div align="center">

## ğŸŒŸ Â¡Gracias por tu interÃ©s en ProyectoIAUE! ğŸŒŸ

**Si este proyecto te ha sido Ãºtil, no olvides darle una â­ en GitHub**

---

*Desarrollado con â¤ï¸ para el avance de la ConducciÃ³n AutÃ³noma*

</div>
